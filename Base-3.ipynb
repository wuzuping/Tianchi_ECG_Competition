{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "import librosa\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = pd.read_csv('../hf_round2_train.txt', sep='\\t', names = list(range(0,12)))\n",
    "label.columns = ['id','age','gender','f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10','f11']\n",
    "arrythmia = pd.read_csv('../hf_round2_arrythmia.txt',header=None,sep=\"\\t\")#读入心电异常事件列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((34, 1), 34)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrythmia.shape, arrythmia[0].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = dict(zip(arrythmia[0].unique(), range(arrythmia.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20036, 34)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_label = np.zeros((label.shape[0], arrythmia.shape[0])).astype(int)\n",
    "for i in range(label.shape[0]):\n",
    "    dis = list(label.iloc[i, 3:].unique())\n",
    "    dis.pop()\n",
    "    for d in dis:\n",
    "        if type(d) == str:\n",
    "            y_label[i, label_map[d]] = 1\n",
    "y_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"\n",
    "    convert ndarrays in sample to Tensors.\n",
    "    return:\n",
    "        feat(torch.FloatTensor)\n",
    "        label(torch.LongTensor of size batch_size x 1)\n",
    "    \"\"\"\n",
    "    def __call__(self, data):\n",
    "        data = torch.from_numpy(data).type(torch.FloatTensor)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import MaxPool2d\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2):\n",
    "        super().__init__()\n",
    "        self.gamma = 2\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        assert target.size() == input.size()\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "        invporbs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        loss = (invporbs * self.gamma).exp() * loss\n",
    "        return loss.sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../hf_round2_train/100001.txt', sep=' ')\n",
    "data['III'] = data['II']-data['I']\n",
    "data['aVR']=-(data['II']+data['I'])/2\n",
    "data['aVL']=(data['I']-data['II'])/2\n",
    "data['aVF']=(data['II']-data['I'])/2\n",
    "data = data.astype('float').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>f11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001.txt</td>\n",
       "      <td>47.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>窦性心律</td>\n",
       "      <td>T波改变</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003.txt</td>\n",
       "      <td>40.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>窦性心动过速</td>\n",
       "      <td>T波改变</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100005.txt</td>\n",
       "      <td>42.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>窦性心律</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100007.txt</td>\n",
       "      <td>21.0</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>窦性心动过速</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100009.txt</td>\n",
       "      <td>49.0</td>\n",
       "      <td>MALE</td>\n",
       "      <td>窦性心动过速</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id   age  gender      f3    f4   f5   f6   f7   f8  f9  f10  f11\n",
       "0  100001.txt  47.0    MALE    窦性心律  T波改变  NaN  NaN  NaN  NaN NaN  NaN  NaN\n",
       "1  100003.txt  40.0    MALE  窦性心动过速  T波改变  NaN  NaN  NaN  NaN NaN  NaN  NaN\n",
       "2  100005.txt  42.0    MALE    窦性心律   NaN  NaN  NaN  NaN  NaN NaN  NaN  NaN\n",
       "3  100007.txt  21.0  FEMALE  窦性心动过速   NaN  NaN  NaN  NaN  NaN NaN  NaN  NaN\n",
       "4  100009.txt  49.0    MALE  窦性心动过速   NaN  NaN  NaN  NaN  NaN NaN  NaN  NaN"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label['age'] = label['age'].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label['gender'] = label['gender'].fillna('MISS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with Pool(32) as p:\n",
    " #   Feats = p.map(Get_Feat, list(label['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biosppy.signals import ecg\n",
    "from pyentrp import entropy as ent\n",
    "import pywt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WTfilt_1d(sig):\n",
    "    \"\"\"\n",
    "    # 使用小波变换对单导联ECG滤波\n",
    "    # 参考：Martis R J, Acharya U R, Min L C. ECG beat classification using PCA, LDA, ICA and discrete\n",
    "    wavelet transform[J].Biomedical Signal Processing and Control, 2013, 8(5): 437-448.\n",
    "    :param sig: 1-D numpy Array，单导联ECG\n",
    "    :return: 1-D numpy Array，滤波后信号\n",
    "    \"\"\"\n",
    "    coeffs = pywt.wavedec(sig, 'db6', level=9)\n",
    "    coeffs[-1] = np.zeros(len(coeffs[-1]))\n",
    "    coeffs[-2] = np.zeros(len(coeffs[-2]))\n",
    "    coeffs[0] = np.zeros(len(coeffs[0]))\n",
    "    sig_filt = pywt.waverec(coeffs, 'db6')\n",
    "    return sig_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManFeat_HRV(object):\n",
    "    \"\"\"\n",
    "        针对一条记录的HRV特征提取， 以II导联为基准\n",
    "    \"\"\"\n",
    "    FEAT_DIMENSION = 9\n",
    "\n",
    "    def __init__(self, sig, fs=250.0):\n",
    "        assert len(sig.shape) == 1, 'The signal must be 1-dimension.'\n",
    "        assert sig.shape[0] >= fs * 6, 'The signal must >= 6 seconds.'\n",
    "        self.sig = WTfilt_1d(sig)\n",
    "        self.fs = fs\n",
    "        self.rpeaks, = ecg.hamilton_segmenter(signal=self.sig, sampling_rate=self.fs)\n",
    "        self.rpeaks, = ecg.correct_rpeaks(signal=self.sig, rpeaks=self.rpeaks,\n",
    "                                         sampling_rate=self.fs)\n",
    "        self.RR_intervals = np.diff(self.rpeaks)\n",
    "        self.dRR = np.diff(self.RR_intervals)\n",
    "    \n",
    "    def __get_sdnn(self):  # 计算RR间期标准差\n",
    "        return np.array([np.std(self.RR_intervals)])\n",
    "\n",
    "    def __get_maxRR(self):  # 计算最大RR间期\n",
    "        return np.array([np.max(self.RR_intervals)])\n",
    "\n",
    "    def __get_minRR(self):  # 计算最小RR间期\n",
    "        return np.array([np.min(self.RR_intervals)])\n",
    "\n",
    "    def __get_meanRR(self):  # 计算平均RR间期\n",
    "        return np.array([np.mean(self.RR_intervals)])\n",
    "\n",
    "    def __get_Rdensity(self):  # 计算R波密度\n",
    "        return np.array([(self.RR_intervals.shape[0] + 1) \n",
    "                         / self.sig.shape[0] * self.fs])\n",
    "\n",
    "    def __get_pNN50(self):  # 计算pNN50\n",
    "        return np.array([self.dRR[self.dRR >= self.fs*0.05].shape[0] \n",
    "                         / self.RR_intervals.shape[0]])\n",
    "\n",
    "    def __get_RMSSD(self):  # 计算RMSSD\n",
    "        return np.array([np.sqrt(np.mean(self.dRR*self.dRR))])\n",
    "    \n",
    "    def __get_SampEn(self):  # 计算RR间期采样熵\n",
    "        sampEn = ent.sample_entropy(self.RR_intervals, \n",
    "                                  2, 0.2 * np.std(self.RR_intervals))\n",
    "        for i in range(len(sampEn)):\n",
    "            if np.isnan(sampEn[i]):\n",
    "                sampEn[i] = -2\n",
    "            if np.isinf(sampEn[i]):\n",
    "                sampEn[i] = -1\n",
    "        return sampEn\n",
    "\n",
    "    def extract_features(self):  # 提取HRV所有特征\n",
    "        if len(self.RR_intervals) == 0 or len(self.dRR) == 0:\n",
    "            features =  np.zeros((ManFeat_HRV.FEAT_DIMENSION,))\n",
    "        else:\n",
    "            features = np.concatenate((self.__get_sdnn(),\n",
    "                    self.__get_maxRR(),\n",
    "                    self.__get_minRR(),\n",
    "                    self.__get_meanRR(),\n",
    "                    self.__get_Rdensity(),\n",
    "                    self.__get_pNN50(),\n",
    "                    self.__get_RMSSD(),\n",
    "                    self.__get_SampEn(),\n",
    "                    ))\n",
    "        assert features.shape[0] == ManFeat_HRV.FEAT_DIMENSION\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RR(data):\n",
    "    result = []\n",
    "    for i in range(12):\n",
    "        arr = data[:,i]\n",
    "        s = ManFeat_HRV(arr, fs=250)\n",
    "        fea = s.extract_features()\n",
    "        result.append(fea)\n",
    "    result = np.concatenate(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RR_Feat(Id):\n",
    "    data = pd.read_csv('../hf_round2_train/'+Id, sep=' ')\n",
    "    data['III'] = data['II']-data['I']\n",
    "    data['aVR']=-(data['II']+data['I'])/2\n",
    "    data['aVL']=(data['I']-data['II'])/2\n",
    "    data['aVF']=(data['II']-data['I'])/2\n",
    "    data = data.astype('float').values\n",
    "    data = resample(data, 2500)\n",
    "    feas = get_RR(data)\n",
    "    return feas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20036, 108)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with Pool(12) as p:\n",
    "    Feat_RR = p.map(RR_Feat, list(label['id']))\n",
    "Feat_RR = np.stack(Feat_RR)\n",
    "Feat_RR.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_mean = label['age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_std = label['age'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, fname, mode,  y, feat, transform=None):\n",
    "        self.fname = fname\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.y = y\n",
    "        self.feat = feat\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.fname.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.fname['id'].iloc[idx]\n",
    "        age = self.fname['age'].iloc[idx]\n",
    "        age = (age -age_mean)/age_std\n",
    "        gender = self.fname['gender'].iloc[idx]\n",
    "        if self.mode=='train':\n",
    "            filepath = os.path.join('../hf_round2_train/', filename.split('.')[0]+'.txt')\n",
    "        data = pd.read_csv(filepath, sep=' ')\n",
    "        data['III'] = data['II']-data['I']\n",
    "        data['aVR']=-(data['II']+data['I'])/2\n",
    "        data['aVL']=(data['I']-data['II'])/2\n",
    "        data['aVF']=(data['II']-data['I'])/2\n",
    "        data = data.astype('float').values\n",
    "        data = resample(data, 2500).transpose()\n",
    "        fea = self.feat[idx].reshape(1,-1)\n",
    "        fea2 = np.zeros((4,))\n",
    "        fea2[0] = age\n",
    "        if gender == 'MALE':\n",
    "            fea2[1] = 1\n",
    "        if gender == 'FEMALE':\n",
    "            fea2[2] = 1\n",
    "        if gender == 'MISS':\n",
    "            fea2[3] = 1\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "            fea = self.transform(fea)\n",
    "            fea2 = self.transform(fea2)\n",
    "        if self.mode == 'train':\n",
    "            L = self.y[idx]\n",
    "            return data, fea, fea2, L\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Data(label, 'train',  y_label,Feat_RR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12, 2500), (1, 108), (4,), (34,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[0][0].shape, s[0][1].shape, s[0][2].shape, s[0][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from resnet import resnet34, resnet50, resnet18\n",
    "#from densenet121 import densenet121\n",
    "#from densenet import densenet121\n",
    "#from img_net import img_net\n",
    "from senet import se_resnet50, se_resnext50_32x4d, se_resnet101\n",
    "#from resxnet32 import resnext101_32x4d_features\n",
    "#from resnet_no_head import resnetm34\n",
    "#from eff import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, base):\n",
    "        super(Model, self).__init__()\n",
    "        self.base = base\n",
    "        self.bn1 = nn.BatchNorm1d(1)\n",
    "        self.classifier = nn.Linear(2160, 34)\n",
    "    def forward(self, x1, x2, x3):\n",
    "        x1 = self.base(x1)\n",
    "        x2 = self.bn1(x2)\n",
    "        x2 = x2.view(x2.size(0), -1)\n",
    "        x = torch.cat([x1,x2,x3], 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91023923, 0.14235587, 0.36067376, 0.12263167, 0.14235587,\n",
       "       0.20887764, 0.24044917, 0.25308487, 0.36067376, 0.17680365,\n",
       "       0.20178267, 0.31066747, 0.15843505, 0.31066747, 0.28126641,\n",
       "       0.28853901, 0.31892899, 0.16595125, 0.11670455, 0.32351545,\n",
       "       0.10918041, 0.51389834, 0.17542494, 0.36067376, 0.2969742 ,\n",
       "       0.28357849, 0.15880302, 0.17393141, 0.14698306, 0.16568687,\n",
       "       0.11770287, 0.20677041, 0.28126641, 0.24423934])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc = y_label.sum(axis=0)\n",
    "weight = 1.0/np.log(wc)\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train, x_val, train_label,val_label,fold,weight,Model,feat):\n",
    "    num_epochs = 68\n",
    "    batch_size = 32\n",
    "    test_batch_size = 32\n",
    "    lr = 1e-3\n",
    "    eta_min = 1e-5\n",
    "    t_max = 25\n",
    "    \n",
    "    num_classes = 34\n",
    "    \n",
    "    train_dataset = Data(x_train, 'train',train_label, feat, transform=transforms.Compose([ToTensor()]))\n",
    "    valid_dataset = Data(x_val, 'train', val_label, feat, transform=transforms.Compose([ToTensor()]))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=20)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=test_batch_size, shuffle=False, num_workers=20)\n",
    "    model = Model.cuda()\n",
    "    #criterion = FocalLoss().cuda()\n",
    "    w = torch.tensor(weight, dtype=torch.float).cuda()\n",
    "    criterion = nn.MultiLabelSoftMarginLoss(weight=w).cuda()\n",
    "    val_criterion = nn.BCEWithLogitsLoss().cuda()\n",
    "    optimizer = Adam(params=model.parameters(), lr=lr, amsgrad=False)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(factor=0.1, mode='max', min_lr=eta_min, patience=3, optimizer=optimizer)\n",
    "    #scheduler = CosineAnnealingLR(optimizer, T_max=t_max, eta_min=eta_min)\n",
    "    best_epoch = -1\n",
    "    best_loss = 0\n",
    "    mb = range(num_epochs)\n",
    "    for epoch in mb:\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "\n",
    "        for x_batch, x1, x2, y_batch in train_loader:\n",
    "            #x_batch, y_batch, lam = mixup_data(x_batch.cuda(), y_batch.cuda(), alpha=1.0)\n",
    "            preds = model(x_batch.cuda(),x1.cuda(), x2.cuda())\n",
    "            loss = criterion(preds, y_batch.cuda().float())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        valid_preds = np.zeros((len(x_val), num_classes))\n",
    "        avg_val_loss = 0.\n",
    "        pred_list = []\n",
    "        label_list = []\n",
    "        for i, (x_batch, x1, x2, y_batch) in enumerate(valid_loader):\n",
    "            preds = model(x_batch.cuda(), x1.cuda(), x2.cuda()).detach()\n",
    "            loss = criterion(preds, y_batch.cuda().float())\n",
    "\n",
    "            preds = torch.sigmoid(preds)\n",
    "            valid_preds[i * test_batch_size: (i+1) * test_batch_size] = preds.cpu().numpy()\n",
    "\n",
    "            avg_val_loss += loss.item() / len(valid_loader)\n",
    "            predictions = preds.cpu().numpy()\n",
    "            label = y_batch.cpu().numpy()\n",
    "            pred_list.append(predictions)\n",
    "            label_list.append(label)\n",
    "        pred_list = np.concatenate(pred_list).reshape(-1)\n",
    "        label_list = np.concatenate(label_list).reshape(-1)\n",
    "        lss = []\n",
    "        for threshold in np.linspace(0, 1, 101):\n",
    "            s = f1_score(label_list, (pred_list>threshold).astype(int))\n",
    "            lss.append((threshold, s))\n",
    "        avg_f1 = sorted(lss, key=lambda x:x[1], reverse=True)[0][1]\n",
    "        threshold = sorted(lss, key=lambda x:x[1], reverse=True)[0][0]\n",
    "            \n",
    "        \n",
    "        \n",
    "        scheduler.step(avg_f1)\n",
    "        #scheduler.step()\n",
    "\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('Epoch %d - avg_train_loss: %f  avg_val_loss: %f  avg_f1:%f threshold:%f time: %f s'%(epoch+1, avg_loss, avg_val_loss,avg_f1, threshold, elapsed))\n",
    "    \n",
    "        if avg_f1 > best_loss:\n",
    "            best_epoch = epoch + 1\n",
    "            best_loss = avg_f1\n",
    "            torch.save(model.state_dict(), 'weight_best_%s.pt'%str(fold))          \n",
    "    return {\n",
    "        'best_epoch': best_epoch,\n",
    "        'best_loss': best_loss\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16027, 12) (16027, 34)\n",
      "Fold 0, Train samples:16027, val samples:4009\n",
      "(16031, 12) (16031, 34)\n",
      "Fold 1, Train samples:16031, val samples:4005\n",
      "(16031, 12) (16031, 34)\n",
      "Fold 2, Train samples:16031, val samples:4005\n",
      "(16026, 12) (16026, 34)\n",
      "Fold 3, Train samples:16026, val samples:4010\n",
      "(16029, 12) (16029, 34)\n",
      "Fold 4, Train samples:16029, val samples:4007\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "mskf = MultilabelStratifiedKFold(n_splits=5, random_state=2018)\n",
    "kf = KFold(n_splits=5, random_state=2018, shuffle=True)\n",
    "for foldNum, (train_split, val_split) in enumerate(mskf.split(label, y_label)):\n",
    "    end = time.time()\n",
    "    # split the dataset for cross-validation\n",
    "    train_fname = label.iloc[train_split]\n",
    "    train_label = y_label[train_split]\n",
    "    val_fname = label.iloc[val_split]\n",
    "    val_label = y_label[val_split]\n",
    "    print(train_fname.shape,train_label.shape)\n",
    "    print(\"Fold {0}, Train samples:{1}, val samples:{2}\".format(foldNum, len(train_fname), len(val_fname)))\n",
    "    #result = train_model(train_fname, val_fname, train_label, val_label,foldNum, weight, Model(se_resnext50_32x4d(num_classes=34, pretrained=None)),Feat_RR )\n",
    "    #result = train_model(train_fname, val_fname, train_label, val_label,foldNum, weight, EfficientNet(1,1,num_classes=34))\n",
    "    #print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestData(Dataset):\n",
    "    def __init__(self, fname,mode, feat,  transform=None):\n",
    "        self.fname = fname\n",
    "        self.transform = transform\n",
    "        self.mode=mode\n",
    "        self.feat = feat\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.fname.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.fname['id'].iloc[idx]\n",
    "        age = self.fname['age'].iloc[idx]\n",
    "        age = (age -age_mean)/age_std\n",
    "        gender = self.fname['gender'].iloc[idx]\n",
    "        if self.mode == 'train':\n",
    "            filepath = os.path.join('../hf_round2_train/', filename.split('.')[0]+'.txt')\n",
    "        else:\n",
    "            filepath = os.path.join('tcdata/hf_round2_testA/', filename.split('.')[0]+'.txt')\n",
    "        data = pd.read_csv(filepath, sep=' ')\n",
    "        data['III'] = data['II']-data['I']\n",
    "        data['aVR']=-(data['II']+data['I'])/2\n",
    "        data['aVL']=(data['I']-data['II'])/2\n",
    "        data['aVF']=(data['II']-data['I'])/2\n",
    "        data = data.astype('float').values\n",
    "        data = resample(data, 2500).transpose()\n",
    "        fea = self.feat[idx].reshape(1,-1)\n",
    "        fea2 = np.zeros((4,))\n",
    "        fea2[0] = age\n",
    "        if gender == 'MALE':\n",
    "            fea2[1] = 1\n",
    "        if gender == 'FEMALE':\n",
    "            fea2[2] = 1\n",
    "        if gender == 'MISS':\n",
    "            fea2[3] = 1\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "            fea = self.transform(fea)\n",
    "            fea2 = self.transform(fea2)\n",
    "        return data, fea,fea2, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#model_conv = resnet34(pretrained=False)\n",
    "#model_conv.conv1 = nn.Conv2d(12, 64, kernel_size=(7,7), stride=(2,2), padding=(3,3), bias=False)\n",
    "#model_conv = nn.Sequential(*([nn.BatchNorm2d(12)]+list(model_conv.children())[:-1]))\n",
    "#model = Model(resnetm34(), model_conv).cuda()\n",
    "model = Model(se_resnext50_32x4d(num_classes=34, pretrained=None)).cuda()\n",
    "valres = []\n",
    "for foldNum, (train_split, val_split) in enumerate(mskf.split(label, y_label)):\n",
    "    end = time.time()\n",
    "    # split the dataset for cross-validation\n",
    "    val_fname = label.iloc[val_split]\n",
    "    val_label = y_label[val_split]\n",
    "    valData = TestData(val_fname, 'train',Feat_RR, transform=transforms.Compose([ToTensor()]))\n",
    "    valloader = DataLoader(valData, batch_size=64, shuffle=False)\n",
    "    model.load_state_dict(torch.load('weight_best_%s.pt'% str(foldNum)))\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    val_outputs = []\n",
    "    val_fnames = []\n",
    "    for images, fea, fea2, fnames in valloader:\n",
    "        preds = torch.sigmoid(model(images.cuda(), fea.cuda(), fea2.cuda()).detach())\n",
    "        val_outputs.append(preds.cpu().numpy())\n",
    "        val_fnames.extend(fnames)\n",
    "\n",
    "    val_preds = pd.DataFrame(data=np.concatenate(val_outputs),\n",
    "                                  index=val_fnames,\n",
    "                                  columns=map(str, range(34)))\n",
    "    val_preds = val_preds.groupby(level=0).mean()\n",
    "    valres.append(val_preds)\n",
    "    print(1)\n",
    "val_predsss = pd.concat([valres[0].reset_index(),valres[1].reset_index(),valres[2].reset_index(),valres[3].reset_index(),valres[4].reset_index()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20036, 34), (20036, 35))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_label.shape,val_predsss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "resall = val_predsss.rename(columns={'index':'id'})\n",
    "kkksksk = pd.merge(label[['id']], resall, how='left', on='id')\n",
    "from sklearn.metrics import f1_score\n",
    "pred_pro = kkksksk.drop('id', axis=1).values\n",
    "pred_pro = pred_pro.reshape(-1)\n",
    "lss = []\n",
    "for i in np.linspace(0, 1, 101):\n",
    "    s = f1_score(y_label.reshape(-1), (pred_pro>i).astype(int))\n",
    "    lss.append((i, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkksksk.to_csv('../se_resnet50_32x4d_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.53, 0.9187668383180438)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lss, key=lambda x:x[1], reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks2 = kkksksk.copy()\n",
    "map2 = dict(zip(range(34), [0.49]*34))\n",
    "for j in range(34):\n",
    "    res = []\n",
    "    for th in np.linspace(0, 1, 101):\n",
    "        for i in range(34):\n",
    "            if i == j:\n",
    "                ks2[str(i)] = (kkksksk[str(i)] > th).astype(int)\n",
    "            else:\n",
    "                ks2[str(i)] = (kkksksk[str(i)] > map2[i]).astype(int)\n",
    "        s = f1_score(y_label.reshape(-1), ks2.drop('id', axis=1).values.reshape(-1))\n",
    "        res.append((th, s))\n",
    "    map2[j] = sorted(res, key=lambda x:x[1], reverse=True)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map2 = {0: 0.1,1: 0.48,2: 0.88,3: 0.43,4: 0.44,5: 0.5700000000000001,6: 0.59,7: 0.86,8: 0.99,9: 0.67,10: 0.35000000000000003,\n",
    " 11: 0.8200000000000001,12: 0.5700000000000001,13: 0.8200000000000001,14: 0.76,15: 0.86,16: 0.87,17: 0.48,18: 0.54,19: 0.96,\n",
    " 20: 0.48,21: 0.6,22: 0.58,23: 0.47000000000000003,24: 0.49,25: 0.96,26: 0.55,27: 0.56,28: 0.65,29: 0.47000000000000003,30: 0.47000000000000003,\n",
    " 31: 0.6900000000000001,32: 0.98,33: 0.81}\n",
    "map2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(34):\n",
    "    ks2[str(i)] = (kkksksk[str(i)] > map2[i]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_label.reshape(-1), ks2.drop('id', axis=1).values.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(lss, key=lambda x:x[1], reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestData(Dataset):\n",
    "    def __init__(self, fname,mode,transform=None):\n",
    "        self.fname = fname\n",
    "        self.transform = transform\n",
    "        self.mode=mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.fname.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.fname['id'].iloc[idx]\n",
    "        if self.mode == 'train':\n",
    "            filepath = os.path.join('hf_round2_train/', filename.split('.')[0]+'.txt')\n",
    "        else:\n",
    "            filepath = os.path.join('tcdata/hf_round2_testA/', filename.split('.')[0]+'.txt')\n",
    "        data = pd.read_csv(filepath, sep=' ')\n",
    "        data['III'] = data['II']-data['I']\n",
    "        data['aVR']=-(data['II']+data['I'])/2\n",
    "        data['aVL']=(data['I']-data['II'])/2\n",
    "        data['aVF']=(data['II']-data['I'])/2\n",
    "        data = data.astype('float').values\n",
    "        data = resample(data, 2500)\n",
    "        #mean = np.mean(data)\n",
    "        #std = np.std(data)\n",
    "        #data = (data -mean)/(std+1e-8)\n",
    "        # if random.random() > 0.5:\n",
    "        #    data = np.flipud(data).copy()\n",
    "        #if random.random() > 0.5:\n",
    "        #    start = random.randint(0, 2250)\n",
    "        #    data[start:start+250,:] = 0\n",
    "        img = data.transpose()\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet34().cuda()\n",
    "valres = []\n",
    "for foldNum, (train_split, val_split) in enumerate(mskf.split(label, y_label)):\n",
    "    end = time.time()\n",
    "    # split the dataset for cross-validation\n",
    "    val_fname = label.iloc[val_split]\n",
    "    val_label = y_label[val_split]\n",
    "    valData = TestData(val_fname, 'train',transform=transforms.Compose([ToTensor()]))\n",
    "    valloader = DataLoader(valData, batch_size=64, shuffle=False)\n",
    "    model.load_state_dict(torch.load('9211/weight_best_%s.pt'% str(foldNum)))\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    val_outputs = []\n",
    "    val_fnames = []\n",
    "    for images, fnames in valloader:\n",
    "        preds = torch.sigmoid(model(images.cuda()).detach())\n",
    "        val_outputs.append(preds.cpu().numpy())\n",
    "        val_fnames.extend(fnames)\n",
    "\n",
    "    val_preds = pd.DataFrame(data=np.concatenate(val_outputs),\n",
    "                                  index=val_fnames,\n",
    "                                  columns=map(str, range(34)))\n",
    "    val_preds = val_preds.groupby(level=0).mean()\n",
    "    valres.append(val_preds)\n",
    "    print(1)\n",
    "val_predsss2 = pd.concat([valres[0].reset_index(),valres[1].reset_index(),valres[2].reset_index(),valres[3].reset_index(),valres[4].reset_index()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pd.concat([val_predsss[['index']], (val_predsss2.drop('index', axis=1) + val_predsss.drop('index', axis=1))/2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resall = total.rename(columns={'index':'id'})\n",
    "kkksksk = pd.merge(label[['id']], resall, how='left', on='id')\n",
    "from sklearn.metrics import f1_score\n",
    "pred_pro = kkksksk.drop('id', axis=1).values\n",
    "pred_pro = pred_pro.reshape(-1)\n",
    "lss = []\n",
    "for i in np.linspace(0, 1, 101):\n",
    "    s = f1_score(y_label.reshape(-1), (pred_pro>i).astype(int))\n",
    "    lss.append((i, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(lss, key=lambda x:x[1], reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predsss = pd.concat([valres[0].reset_index(),valres[1].reset_index(),valres[2].reset_index(),valres[3].reset_index(),valres[4].reset_index()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpredsss = (testres[0]+testres[1]+testres[2]+testres[3]+testres[4])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testresult = testpredsss.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predsss = val_predsss.rename(columns={'index':'id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkksksk = pd.merge(label[['id']], val_predsss, how='left', on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map_reverse = {v:k for (k,v) in label_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hf_round1_subB_noDup_rename.txt', 'r') as f:\n",
    "    su = f.readlines()\n",
    "    su = [x[:-1] for x in su]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_pro = kkksksk.drop('id', axis=1).values\n",
    "pred_pro = pred_pro.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lss = []\n",
    "for i in np.linspace(0, 1, 101):\n",
    "    s = f1_score(y_label.reshape(-1), (pred_pro>i).astype(int))\n",
    "    lss.append((i, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(lss, key=lambda x:x[1], reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkksksk.to_csv('ensemble/val_pred_dense121_baseline_2_dense_has_A.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testresult.to_csv('ensemble/testB_pred_dense121_baseline_2_dense_has_B.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = (testres[0]+testres[1]+testres[2]+testres[3]+testres[4])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = (test2 > 0.48).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = test2.reset_index().rename(columns={'index':'id'})\n",
    "kkk2 = pd.merge(subB, test2, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('subB.txt', 'w') as f:\n",
    "    for i in range(kkk2.shape[0]):\n",
    "        data = kkk2.iloc[i]\n",
    "        res = su[i]\n",
    "        for j in range(55):\n",
    "            if data[str(j)] == 1:\n",
    "                res += '\\t'\n",
    "                res += label_map_reverse[j]\n",
    "        res += '\\n'\n",
    "        f.write(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l subB.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
